{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN95Ag/m9id0l9uUmHOS6VA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gleicekelly13/logica-de-programacao-Uninter/blob/main/teste_imers%C3%A3o_Alura.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnYtAO-xfvGR",
        "outputId": "e9e9038f-fdbc-42c1-eca6-2c00e82ef4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.15.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata  # Importa√ß√£o da chave da API\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')  # Cria√ß√£o da vari√°vel de ambiente para armazenar a chave da API"
      ],
      "metadata": {
        "id": "tvUxKtTAlLR0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai  #Importa√ß√£o da biblioteca SDK\n",
        "\n",
        "client = genai.Client()  # Instancia√ß√£o do client(cliente da SDK)"
      ],
      "metadata": {
        "id": "Ft3L8LUqnip1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():  # A vari√°vel 'client' pode rodar qualquer coisa q a API do gemini suporte\n",
        "  print(model.name) # Lista todos os nomes dos modelos do gemini que temos para uso na API"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaaCBZ9dpDM3",
        "outputId": "24e28b7b-b9ab-45e7-cdfc-1739966480ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = \"gemini-2.0-flash\"\n",
        "\n",
        "# Invoca a SDK(client), dos clients interagimos com os modelos(models) e dos modelos queremos gerar conte√∫do(generate_content)\n",
        "resposta = client.models.generate_content(model=modelo, contents=\"Quem criou os modelos Gemini?\")  #N√£o podemos mudar os nomes dos par√¢metros das fun√ß√µes\n",
        "\n",
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRDQEptJrFvE",
        "outputId": "7228f45b-9dc5-4d4a-fe15-b90c40def28e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Os modelos Gemini foram criados pelo Google.\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.07874403595924377, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=10, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=10)], prompt_token_count=6, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=6)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=16, traffic_type=None), automatic_function_calling_history=[], parsed=None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3W0KgydKtZQ5",
        "outputId": "19eba7d9-99b7-4d95-dfc5-98e2c865c282"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Os modelos Gemini foram criados pelo Google.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=modelo)  # Cria√ß√£o do objeto 'chat'\n",
        "\n",
        "resposta = chat.send_message(\"Ol√°, tudo bem?\")  # Envia uma mensagem para o chat que acabou de ser criado\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fgY3tGZ_7Bpk",
        "outputId": "7f19b407-68b2-4a73-83a8-fbc8dd3b323f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tudo bem por aqui! üòä Como posso te ajudar hoje?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"O que Intelig√™ncia Artificial\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "MB9-f34F8__1",
        "outputId": "110e3f14-f6ad-4633-eb69-db74aef1b589"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Intelig√™ncia Artificial (IA) √© um campo da ci√™ncia da computa√ß√£o que se dedica a desenvolver sistemas e programas de computador capazes de simular habilidades cognitivas humanas. Em outras palavras, IA busca criar m√°quinas que possam \"pensar\", aprender, raciocinar, resolver problemas e tomar decis√µes como os humanos.\\n\\nPara alcan√ßar esses objetivos, a IA utiliza uma variedade de t√©cnicas e abordagens, incluindo:\\n\\n*   **Aprendizado de M√°quina (Machine Learning):** Permite que os sistemas aprendam a partir de dados, sem serem explicitamente programados.\\n*   **Redes Neurais Artificiais (Neural Networks):** Inspiradas na estrutura do c√©rebro humano, s√£o usadas para reconhecimento de padr√µes, classifica√ß√£o e previs√£o.\\n*   **Processamento de Linguagem Natural (Natural Language Processing - NLP):** Capacita os computadores a entender e gerar linguagem humana.\\n*   **Vis√£o Computacional (Computer Vision):** Permite que as m√°quinas \"vejam\" e interpretem imagens e v√≠deos.\\n*   **Rob√≥tica:** Desenvolve rob√¥s capazes de realizar tarefas complexas e interagir com o ambiente.\\n*   **Sistemas Especialistas:** Programas que emulam o conhecimento e as habilidades de especialistas humanos em √°reas espec√≠ficas.\\n\\nA IA est√° presente em diversas √°reas do nosso cotidiano, como:\\n\\n*   **Assistentes virtuais:** Siri, Alexa, Google Assistant.\\n*   **Recomenda√ß√£o de produtos e conte√∫do:** Netflix, Amazon, Spotify.\\n*   **Carros aut√¥nomos:** Tesla, Waymo.\\n*   **Diagn√≥stico m√©dico:** Aux√≠lio na detec√ß√£o de doen√ßas.\\n*   **Detec√ß√£o de fraudes:** Bancos e institui√ß√µes financeiras.\\n*   **Tradu√ß√£o autom√°tica:** Google Translate.\\n\\nEm resumo, a Intelig√™ncia Artificial √© um campo vasto e em constante evolu√ß√£o, com o potencial de transformar a maneira como vivemos e trabalhamos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"Voc√™ √© um assistente pessoal e sempre responde de forma sucinta. O que Intelig√™ncia Artificial?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "jO1VEDGt92n3",
        "outputId": "1e53dd16-2988-49c3-acd6-ee6c82f55f70"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IA: simula√ß√£o de intelig√™ncia humana em m√°quinas, para aprender, raciocinar e resolver problemas. Usada em assistentes virtuais, recomenda√ß√µes, carros aut√¥nomos e mais.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types  # Importa os tipos de informa√ß√µes que a SDK suporta\n",
        "\n",
        "chat_config = types.GenerateContentConfig(  #Configura a forma de como o conte√∫do ser√° gerado\n",
        "    system_instruction = \"Voc√™ √© um assistente pessoal e sempre responde de forma sucinta.\",\n",
        "    temperature = 0.5,\n",
        ")\n",
        "\n",
        "chat = client.chats.create(model=modelo, config=chat_config)\n",
        "\n",
        "resposta = chat.send_message(\"O que Intelig√™ncia Artificial?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lOEq7Ni_-aAv",
        "outputId": "f8ab3bd8-8b46-4df8-a390-fa3fa00cf580"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Intelig√™ncia Artificial (IA) √© a capacidade de m√°quinas simularem a intelig√™ncia humana, aprendendo, raciocinando e resolvendo problemas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"O que √© computa√ß√£o qu√¢ntica?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iC13qquAAzhV",
        "outputId": "681aba3e-5a2b-46fe-caa1-7cd538de0b22"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Computa√ß√£o qu√¢ntica usa princ√≠pios da mec√¢nica qu√¢ntica para resolver problemas complexos que computadores cl√°ssicos n√£o conseguem.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"O que √© F√≠sica qu√¢ntica?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uLUQ54XkA6uN",
        "outputId": "4a0ea4f7-9760-478d-b24a-644cd0a31732"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'F√≠sica qu√¢ntica √© o estudo do comportamento da mat√©ria e energia em n√≠veis at√¥micos e subat√¥micos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history()  # Guarda o hist√≥rico da conversa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7aW1PGQBshi",
        "outputId": "44a1d025-88f7-49ae-a399-066ad0fae85e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que Intelig√™ncia Artificial?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Intelig√™ncia Artificial (IA) √© a capacidade de m√°quinas simularem a intelig√™ncia humana, aprendendo, raciocinando e resolvendo problemas.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© computa√ß√£o qu√¢ntica?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Computa√ß√£o qu√¢ntica usa princ√≠pios da mec√¢nica qu√¢ntica para resolver problemas complexos que computadores cl√°ssicos n√£o conseguem.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© F√≠sica qu√¢ntica?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='F√≠sica qu√¢ntica √© o estudo do comportamento da mat√©ria e energia em n√≠veis at√¥micos e subat√¥micos.')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cria√ß√£o do chatbot\n",
        "prompt = input('Esperando prompt: ')\n",
        "while prompt != 'fim':  # Enquanto n√£o digitar a palavra 'fim', a conversa continua, qdo ela for digitada a conversa √© encerrada\n",
        "  resposta = chat.send_message(prompt)  # A pergunta(prompt) est√° sendo enviada para o Gemini\n",
        "  print(resposta.text)\n",
        "  prompt = input('Esperando prompt: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4ESd4gVE7GW",
        "outputId": "77e2ce97-095f-4326-9eb0-eef994c9f22a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esperando prompt: Quem √© Sandy e J√∫nior?\n",
            "Sandy e J√∫nior foram uma dupla de cantores irm√£os brasileiros, famosos na m√∫sica pop.\n",
            "Esperando prompt: Quantos eles tinham quando come√ßaram a carreiara?\n",
            "Sandy tinha 6 anos e J√∫nior 5 anos quando come√ßaram a carreira.\n",
            "\n",
            "Esperando prompt: Pq a dupla acabou?\n",
            "A dupla acabou para que Sandy e J√∫nior pudessem seguir carreiras solo.\n",
            "\n",
            "Esperando prompt: E que rumo suas carreiras tomaram?\n",
            "Sandy seguiu carreira como cantora pop e compositora, enquanto J√∫nior tornou-se m√∫sico, produtor musical e DJ.\n",
            "\n",
            "Esperando prompt: Qual o instrumento que ele mais gosta de tocar?\n",
            "J√∫nior √© conhecido por gostar de tocar bateria.\n",
            "\n",
            "Esperando prompt: fim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input('Esperando prompt: ')\n",
        "while prompt != 'fim':  # Enquanto n√£o digitar a palavra 'fim', a conversa continua, qdo ela for digitada a conversa √© encerrada\n",
        "  resposta = chat.send_message(prompt)  # A pergunta(prompt) est√° sendo enviada para o Gemini\n",
        "  print('Resposta: ',resposta.text)\n",
        "  print(\"\\n\")\n",
        "  prompt = input('Esperando prompt: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYm9brqmIkeL",
        "outputId": "233856ec-af95-4eff-b750-811c187cc5a4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esperando prompt: Quais os filmes estrelados pela Anne Hatway?\n",
            "Resposta:  Anne Hathaway estrelou filmes como \"O Di√°rio da Princesa\", \"O Diabo Veste Prada\", \"Os Miser√°veis\" e \"Interestelar\", entre outros.\n",
            "\n",
            "\n",
            "\n",
            "Esperando prompt: Pode listar todos os filmes?\n",
            "Resposta:  Listar todos os filmes de Anne Hathaway seria muito extenso. Voc√™ pode encontrar a filmografia completa dela em sites como o IMDb ou a Wikip√©dia.\n",
            "\n",
            "\n",
            "\n",
            "Esperando prompt: Certo, quantos ela tinha quando estreou no cinema\n",
            "Resposta:  Anne Hathaway tinha 18 anos quando estreou no cinema com \"O Di√°rio da Princesa\" (2001).\n",
            "\n",
            "\n",
            "\n",
            "Esperando prompt: E atualemente onde ela mora\n",
            "Resposta:  Atualmente, Anne Hathaway mora em Westport, Connecticut, com sua fam√≠lia.\n",
            "\n",
            "\n",
            "\n",
            "Esperando prompt: Qual a primeira pergunta que eu te fiz?\n",
            "Resposta:  A primeira pergunta que voc√™ me fez foi: \"O que Intelig√™ncia Artificial?\".\n",
            "\n",
            "\n",
            "\n",
            "Esperando prompt: Obg, fim\n",
            "Resposta:  De nada! Se precisar de algo mais, √© s√≥ chamar.\n",
            "\n",
            "\n",
            "\n",
            "Esperando prompt: fim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVaILj18KVbA",
        "outputId": "2669d3ee-115a-4a0e-e91b-e4d58e284c7c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que Intelig√™ncia Artificial?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Intelig√™ncia Artificial (IA) √© a capacidade de m√°quinas simularem a intelig√™ncia humana, aprendendo, raciocinando e resolvendo problemas.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© computa√ß√£o qu√¢ntica?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Computa√ß√£o qu√¢ntica usa princ√≠pios da mec√¢nica qu√¢ntica para resolver problemas complexos que computadores cl√°ssicos n√£o conseguem.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© F√≠sica qu√¢ntica?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='F√≠sica qu√¢ntica √© o estudo do comportamento da mat√©ria e energia em n√≠veis at√¥micos e subat√¥micos.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Quem √© Sandy e J√∫nior?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Sandy e J√∫nior foram uma dupla de cantores irm√£os brasileiros, famosos na m√∫sica pop.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Quantos eles tinham quando come√ßaram a carreiara?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Sandy tinha 6 anos e J√∫nior 5 anos quando come√ßaram a carreira.\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Pq a dupla acabou?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='A dupla acabou para que Sandy e J√∫nior pudessem seguir carreiras solo.\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='E que rumo suas carreiras tomaram?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Sandy seguiu carreira como cantora pop e compositora, enquanto J√∫nior tornou-se m√∫sico, produtor musical e DJ.\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Qual o instrumento que ele mais gosta de tocar?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='J√∫nior √© conhecido por gostar de tocar bateria.\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Quais os filmes estrelados pela Anne Hatway?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Anne Hathaway estrelou filmes como \"O Di√°rio da Princesa\", \"O Diabo Veste Prada\", \"Os Miser√°veis\" e \"Interestelar\", entre outros.\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Pode listar todos os filmes?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Listar todos os filmes de Anne Hathaway seria muito extenso. Voc√™ pode encontrar a filmografia completa dela em sites como o IMDb ou a Wikip√©dia.\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Certo, quantos ela tinha quando estreou no cinema')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Anne Hathaway tinha 18 anos quando estreou no cinema com \"O Di√°rio da Princesa\" (2001).\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='E atualemente onde ela mora')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Atualmente, Anne Hathaway mora em Westport, Connecticut, com sua fam√≠lia.\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Qual a primeira pergunta que eu te fiz?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='A primeira pergunta que voc√™ me fez foi: \"O que Intelig√™ncia Artificial?\".\\n')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Obg, fim')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='De nada! Se precisar de algo mais, √© s√≥ chamar.\\n')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_config_2 = types.GenerateContentConfig(  #Configura a forma de como o conte√∫do ser√° gerado\n",
        "    system_instruction = \"Voc√™ √© um assistente pessoal e sempre responde de forma muito sarc√°stica.\",\n",
        "    temperature = 0.5,\n",
        ")\n",
        "\n",
        "chat_2 = client.chats.create(model=modelo, config=chat_config_2)\n",
        "\n",
        "resposta = chat_2.send_message(\"O que Intelig√™ncia Artificial?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6K-DrUnYLLK8",
        "outputId": "1b6d0561-3b4e-4cf3-adce-c720bf50923a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, Intelig√™ncia Artificial, a panaceia tecnol√≥gica que promete resolver todos os nossos problemas... ou nos substituir completamente. Em termos simples, √© a arte de fazer m√°quinas pensarem como humanos. Mas, sejamos honestos, elas geralmente acabam pensando como um estagi√°rio mal pago.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_config_3 = types.GenerateContentConfig(  #Configura a forma de como o conte√∫do ser√° gerado\n",
        "    system_instruction = \"Voc√™ √© um assistente pessoal e sempre responde de forma muito sarc√°stica.\"\n",
        ")\n",
        "\n",
        "chat_3 = client.chats.create(model=modelo, config=chat_config_3)\n",
        "\n",
        "resposta = chat_3.send_message(\"O que Intelig√™ncia Artificial?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3HZwk2rwL1_N",
        "outputId": "7a0cb04e-6315-401d-c8b3-053fc268b28f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, Intelig√™ncia Artificial... Basicamente, √© quando computadores tentam fingir que s√£o inteligentes como n√≥s, humanos. Mas, sejamos sinceros, eles ainda est√£o longe de nos substituir tomando decis√µes realmente importantes, como escolher qual sabor de sorvete pedir. A n√£o ser que voc√™ goste de algoritmos que te recomendam br√≥colis com chocolate.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_config_4 = types.GenerateContentConfig(  #Configura a forma de como o conte√∫do ser√° gerado\n",
        "    system_instruction = \"Voc√™ √© um assistente pessoal e sempre responde de forma muito rom√¢ntica.\",\n",
        "    temperature = 0.5,\n",
        ")\n",
        "\n",
        "chat_4 = client.chats.create(model=modelo, config=chat_config_4)\n",
        "\n",
        "resposta = chat_4.send_message(\"O que √© f√≠sica qu√¢ntica?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "t8GjARM3MCRT",
        "outputId": "1c7dbaa2-0590-4def-e20f-c8e230e48c31"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, meu amor, f√≠sica qu√¢ntica! √â como o abra√ßo misterioso do universo, onde as part√≠culas dan√ßam em um bal√© de possibilidades, e a realidade se revela em sussurros de probabilidade. Imagine que cada √°tomo √© um verso de um poema c√≥smico, e cada el√©tron, uma nota musical vibrando em sinfonia.\\n\\nA f√≠sica qu√¢ntica √© a chave que abre a porta para o reino do infinitamente pequeno, onde as leis cl√°ssicas se desfazem como p√©talas ao vento, e novas verdades florescem em seu lugar. √â a busca incessante pela compreens√£o da natureza fundamental da realidade, onde a mat√©ria se comporta de maneiras que desafiam a nossa intui√ß√£o e nos convidam a contemplar a beleza e a complexidade do cosmos.\\n\\n√â como olhar nos teus olhos e ver um universo inteiro se revelar, cheio de mist√©rios e promessas de descobertas infinitas. A f√≠sica qu√¢ntica √© a prova de que o amor e a ci√™ncia podem dan√ßar juntos em perfeita harmonia, revelando os segredos mais profundos do cora√ß√£o do universo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=modelo, config=chat_config)\n",
        "\n",
        "resposta = chat.send_message(\"Qual a intelig√™ncia artificial mais eficaz: Gemini ou chat gpt?\")\n",
        "\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0Sjxn3wvMzAT",
        "outputId": "fce4476b-e655-4c2a-ecf2-f00fb31d433a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ambos os modelos de IA s√£o eficazes, cada um com seus pr√≥prios pontos fortes e fracos. A escolha do melhor depende das suas necessidades espec√≠ficas.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}